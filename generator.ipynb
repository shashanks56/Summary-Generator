{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccf67f28",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d6b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Pytorch\n",
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48a9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Transformers\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0c718",
   "metadata": {},
   "source": [
    "# Import and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790bf361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies from transformers\n",
    "#from transformers import PegasusForConditionalGeneration , PegasusTokenizer\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tokenizer\n",
    "#tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "#model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d698271e",
   "metadata": {},
   "source": [
    "# Perform Abstractive Summerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f2c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval.\n",
    "It is primarily concerned with giving computers the ability to support and manipulate human language.\n",
    "It involves processing natural language datasets, \n",
    "such as text corpora or speech corpora, \n",
    "using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. \n",
    "The goal is a computer capable of \"understanding\"[citation needed] the contents of documents, including the contextual nuances of the language within them.\n",
    " To this end, natural language processing often borrows ideas from theoretical linguistics.\n",
    " The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd019fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tokens - number representation of our text\n",
    "tokens = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaaf3b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36e7d83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 4284,  1261,  2196,   143, 72237,   158,   117,   142,   115, 13920,\n",
       "           2672,  3483,   113,   958,  1578,   111,   257, 23513,   107,   168,\n",
       "            117,  3654,  2993,   122,  1132,  4328,   109,   986,   112,   337,\n",
       "            111, 14610,   883,  1261,   107,   168,  3576,  2196,   710,  1261,\n",
       "          24089,   108,   253,   130,  1352,   110, 88758,   132,  3442,   110,\n",
       "          88758,   108,   303,   707,  2613,   121,   936,   132, 65108,   143,\n",
       "            457,   107,   326,   107,  9252,   111,   108,   205,   938,   108,\n",
       "          14849,   952,   121,   936,   158,  1157,   761,  4166,   107,   139,\n",
       "           1000,   117,   114,   958,  3159,   113,   198, 61990, 54151, 73685,\n",
       "            690,  1100,   109,  5169,   113,  2010,   108,   330,   109, 23455,\n",
       "          22598,   113,   109,  1261,   373,   183,   107,   413,   136,   370,\n",
       "            108,   710,  1261,  2196,   432, 10425,   116,   675,   135,  9637,\n",
       "          42108,   107,   139,   552,   137,   237,  7127,  5703,   257,   111,\n",
       "           4275,  4400,   115,   109,  2010,   130,   210,   130, 33076,   111,\n",
       "           5763,   109,  2010,  1118,   107,     1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#given paragraph text in tokens\n",
    "{**tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55cb9826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize\n",
    "summary = model.generate(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c53bf850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,   139,  2560,   113,   710,  1261,  2196,   117,   112,   361,\n",
       "         4328,   109,   986,   112,   630,   111, 14610,   883,  1261,   107,\n",
       "            1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#summary in tokens\n",
    "summary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c12d37c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> The aim of natural language processing is to give computers the ability to understand and manipulate human language.</s>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decoding the summary\n",
    "tokenizer.decode(summary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1296d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
