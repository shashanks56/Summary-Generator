{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccf67f28",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d6b78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (2.3.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (0.18.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Install Pytorch\n",
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48a9941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (4.40.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2024.5.10)\n",
      "Requirement already satisfied: requests in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Install Transformers\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0c718",
   "metadata": {},
   "source": [
    "# Import and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "790bf361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shashank\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#importing dependencies from transformers\n",
    "#from transformers import PegasusForConditionalGeneration , PegasusTokenizer\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ada994a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shashank\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Shashank\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Shashank\\.cache\\huggingface\\hub\\models--google--pegasus-xsum. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "#create tokenizer\n",
    "#tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41ac10ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shashank\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "#model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d698271e",
   "metadata": {},
   "source": [
    "# Perform Abstractive Summerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f2c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval.\n",
    "It is primarily concerned with giving computers the ability to support and manipulate human language.\n",
    "It involves processing natural language datasets, \n",
    "such as text corpora or speech corpora, \n",
    "using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. \n",
    "The goal is a computer capable of \"understanding\"[citation needed] the contents of documents, including the contextual nuances of the language within them.\n",
    " To this end, natural language processing often borrows ideas from theoretical linguistics.\n",
    " The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd019fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#create tokens - number representation of our text\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(text, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongest\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#create tokens - number representation of our text\n",
    "tokens = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaaf3b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36e7d83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 4284,  1261,  2196,   143, 72237,   158,   117,   142,   115, 13920,\n",
       "           2672,  3483,   113,   958,  1578,   111,   257, 23513,   107,   168,\n",
       "            117,  3654,  2993,   122,  1132,  4328,   109,   986,   112,   337,\n",
       "            111, 14610,   883,  1261,   107,   168,  3576,  2196,   710,  1261,\n",
       "          24089,   108,   253,   130,  1352,   110, 88758,   132,  3442,   110,\n",
       "          88758,   108,   303,   707,  2613,   121,   936,   132, 65108,   143,\n",
       "            457,   107,   326,   107,  9252,   111,   108,   205,   938,   108,\n",
       "          14849,   952,   121,   936,   158,  1157,   761,  4166,   107,   139,\n",
       "           1000,   117,   114,   958,  3159,   113,   198, 61990, 54151, 73685,\n",
       "            690,  1100,   109,  5169,   113,  2010,   108,   330,   109, 23455,\n",
       "          22598,   113,   109,  1261,   373,   183,   107,   413,   136,   370,\n",
       "            108,   710,  1261,  2196,   432, 10425,   116,   675,   135,  9637,\n",
       "          42108,   107,   139,   552,   137,   237,  7127,  5703,   257,   111,\n",
       "           4275,  4400,   115,   109,  2010,   130,   210,   130, 33076,   111,\n",
       "           5763,   109,  2010,  1118,   107,     1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#given paragraph text in tokens\n",
    "{**tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55cb9826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize\n",
    "summary = model.generate(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c53bf850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,   139,  2560,   113,   710,  1261,  2196,   117,   112,   361,\n",
       "         4328,   109,   986,   112,   630,   111, 14610,   883,  1261,   107,\n",
       "            1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#summary in tokens\n",
    "summary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c12d37c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> The aim of natural language processing is to give computers the ability to understand and manipulate human language.</s>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decoding the summary\n",
    "tokenizer.decode(summary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1296d096",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2721752837.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    jupyter nbconvert --to script notebook.ipynb\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
